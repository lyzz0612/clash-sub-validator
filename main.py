#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Clashè®¢é˜…èŠ‚ç‚¹æµ‹é€Ÿç­›é€‰ç³»ç»Ÿ
æ”¯æŒé…ç½®æ–‡ä»¶è§£æã€å­—ç¬¦æ›¿æ¢ã€èŠ‚ç‚¹æµ‹é€Ÿå’ŒAPIå†™å…¥
"""

import asyncio
import aiohttp
import yaml
import base64
import json
import time
import re
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
from urllib.parse import urlparse, parse_qs
import os
import sys

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('clash_tester.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class ClashNodeTester:
    def __init__(self, config_file: str = 'config.txt', max_latency: int = 3000, should_keep_old = false):
        """
        åˆå§‹åŒ–æµ‹é€Ÿå™¨

        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
            max_latency: æœ€å¤§å»¶è¿Ÿé˜ˆå€¼ï¼ˆæ¯«ç§’ï¼‰
            should_keep_old: æ˜¯å¦ä¿ç•™æ—§é…ç½®
        """
        self.config_file = config_file
        self.max_latency = max_latency
        self.should_keep_old = should_keep_old
        self.session = None
        self.good_nodes = []

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        connector = aiohttp.TCPConnector(
            limit=100,
            limit_per_host=10,
            ttl_dns_cache=300,
            use_dns_cache=True,
        )
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={'User-Agent': 'ClashNodeTester/1.0'}
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        if self.session:
            await self.session.close()

    def replace_placeholders(self, url: str) -> str:
        """
        æ›¿æ¢URLä¸­çš„å ä½ç¬¦

        Args:
            url: åŸå§‹URL

        Returns:
            æ›¿æ¢åçš„URL
        """
        now = datetime.now()
        replacements = {
            '%Y': now.strftime('%Y'),      # 4ä½æ•°å¹´ä»½
            '%y': now.strftime('%y'),      # 2ä½æ•°å¹´ä»½
            '%m': now.strftime('%m'),      # æœˆä»½
            '%d': now.strftime('%d'),      # æ—¥æœŸ
            '%H': now.strftime('%H'),      # å°æ—¶
            '%M': now.strftime('%M'),      # åˆ†é’Ÿ
            '%S': now.strftime('%S'),      # ç§’
            '%D': now.strftime('%Y%m%d'),  # æ—¥æœŸYYYYMMDD
            '%T': now.strftime('%H%M%S'),  # æ—¶é—´HHMMSS
        }

        result = url
        for placeholder, value in replacements.items():
            result = result.replace(placeholder, value)

        logger.debug(f"URLæ›¿æ¢: {url} -> {result}")
        return result

    def read_config(self) -> List[str]:
        """
        è¯»å–é…ç½®æ–‡ä»¶ä¸­çš„è®¢é˜…é“¾æ¥

        Returns:
            å¤„ç†åçš„è®¢é˜…é“¾æ¥åˆ—è¡¨
        """
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            urls = []
            for line in lines:
                line = line.strip()
                if line and not line.startswith('#'):  # å¿½ç•¥ç©ºè¡Œå’Œæ³¨é‡Š
                    processed_url = self.replace_placeholders(line)
                    urls.append(processed_url)

            logger.info(f"æˆåŠŸè¯»å– {len(urls)} ä¸ªè®¢é˜…é“¾æ¥")
            return urls

        except FileNotFoundError:
            logger.error(f"é…ç½®æ–‡ä»¶ {self.config_file} ä¸å­˜åœ¨")
            return []
        except Exception as e:
            logger.error(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return []

    async def fetch_subscription(self, url: str) -> Optional[List[Dict]]:
        """
        è·å–è®¢é˜…å†…å®¹å¹¶è§£æèŠ‚ç‚¹

        Args:
            url: è®¢é˜…é“¾æ¥

        Returns:
            èŠ‚ç‚¹åˆ—è¡¨
        """
        try:
            logger.info(f"è·å–è®¢é˜…: {url}")
            async with self.session.get(url) as response:
                if response.status != 200:
                    logger.warning(f"è®¢é˜…è·å–å¤±è´¥ {url}: HTTP {response.status}")
                    return None

                content = await response.text()

                # å°è¯•è§£æä¸ºbase64ç¼–ç çš„å†…å®¹
                try:
                    decoded_content = base64.b64decode(content).decode('utf-8')
                    content = decoded_content
                except:
                    pass  # å¦‚æœä¸æ˜¯base64ç¼–ç ï¼Œä½¿ç”¨åŸå†…å®¹

                # è§£æYAMLæ ¼å¼çš„Clashé…ç½®
                try:
                    config = yaml.safe_load(content)
                    if isinstance(config, dict) and 'proxies' in config:
                        nodes = config['proxies']
                        logger.info(f"ä»è®¢é˜…è·å–åˆ° {len(nodes)} ä¸ªèŠ‚ç‚¹")
                        return nodes
                except yaml.YAMLError:
                    pass

                # å¦‚æœä¸æ˜¯YAMLæ ¼å¼ï¼Œå°è¯•è§£æä¸ºèŠ‚ç‚¹é“¾æ¥åˆ—è¡¨
                nodes = []
                for line in content.split('\n'):
                    line = line.strip()
                    if line and (line.startswith('ss://') or line.startswith('vmess://') or
                               line.startswith('trojan://') or line.startswith('vless://')):
                        node = self.parse_node_url(line)
                        if node:
                            nodes.append(node)

                if nodes:
                    logger.info(f"ä»è®¢é˜…è§£æåˆ° {len(nodes)} ä¸ªèŠ‚ç‚¹")
                    return nodes

                logger.warning(f"æ— æ³•è§£æè®¢é˜…å†…å®¹: {url}")
                return None

        except Exception as e:
            logger.error(f"è·å–è®¢é˜…å¤±è´¥ {url}: {e}")
            return None

    def parse_node_url(self, url: str) -> Optional[Dict]:
        """
        è§£æèŠ‚ç‚¹URLä¸ºæ ‡å‡†æ ¼å¼

        Args:
            url: èŠ‚ç‚¹URL

        Returns:
            èŠ‚ç‚¹é…ç½®å­—å…¸
        """
        try:
            if url.startswith('ss://'):
                return self.parse_shadowsocks(url)
            elif url.startswith('vmess://'):
                return self.parse_vmess(url)
            elif url.startswith('trojan://'):
                return self.parse_trojan(url)
            elif url.startswith('vless://'):
                return self.parse_vless(url)
            else:
                return None
        except Exception as e:
            logger.debug(f"è§£æèŠ‚ç‚¹URLå¤±è´¥ {url}: {e}")
            return None

    def parse_shadowsocks(self, url: str) -> Optional[Dict]:
        """è§£æShadowsocks URL"""
        try:
            # ss://base64(method:password)@server:port#name
            url = url[5:]  # ç§»é™¤ ss://
            if '#' in url:
                url, name = url.rsplit('#', 1)
            else:
                name = "SS Node"

            if '@' in url:
                auth, server_port = url.split('@', 1)
                server, port = server_port.split(':', 1)

                # è§£ç è®¤è¯ä¿¡æ¯
                try:
                    auth = base64.b64decode(auth).decode('utf-8')
                    method, password = auth.split(':', 1)
                except:
                    return None

                return {
                    'name': name,
                    'type': 'ss',
                    'server': server,
                    'port': int(port),
                    'cipher': method,
                    'password': password
                }
        except:
            return None

    def parse_vmess(self, url: str) -> Optional[Dict]:
        """è§£æVMess URL"""
        try:
            # vmess://base64(json)
            url = url[8:]  # ç§»é™¤ vmess://
            config_json = base64.b64decode(url).decode('utf-8')
            config = json.loads(config_json)

            return {
                'name': config.get('ps', 'VMess Node'),
                'type': 'vmess',
                'server': config.get('add'),
                'port': int(config.get('port', 443)),
                'uuid': config.get('id'),
                'alterId': int(config.get('aid', 0)),
                'cipher': config.get('scy', 'auto'),
                'network': config.get('net', 'tcp'),
                'tls': config.get('tls') == 'tls'
            }
        except:
            return None

    def parse_trojan(self, url: str) -> Optional[Dict]:
        """è§£æTrojan URL"""
        try:
            # trojan://password@server:port#name
            parsed = urlparse(url)

            return {
                'name': parsed.fragment or 'Trojan Node',
                'type': 'trojan',
                'server': parsed.hostname,
                'port': parsed.port or 443,
                'password': parsed.username,
                'sni': parsed.hostname
            }
        except:
            return None

    def parse_vless(self, url: str) -> Optional[Dict]:
        """è§£æVLESS URL"""
        try:
            # vless://uuid@server:port?params#name
            parsed = urlparse(url)
            params = parse_qs(parsed.query)

            return {
                'name': parsed.fragment or 'VLESS Node',
                'type': 'vless',
                'server': parsed.hostname,
                'port': parsed.port or 443,
                'uuid': parsed.username,
                'flow': params.get('flow', [''])[0],
                'network': params.get('type', ['tcp'])[0],
                'tls': params.get('security', [''])[0] in ['tls', 'reality']
            }
        except:
            return None

    async def test_node_latency(self, node: Dict) -> Optional[int]:
        """
        æµ‹è¯•èŠ‚ç‚¹å»¶è¿Ÿ

        Args:
            node: èŠ‚ç‚¹é…ç½®

        Returns:
            å»¶è¿Ÿå€¼ï¼ˆæ¯«ç§’ï¼‰ï¼ŒNoneè¡¨ç¤ºä¸å¯è®¿é—®
        """
        try:
            server = node.get('server')
            port = node.get('port')

            if not server or not port:
                return None

            start_time = time.time()

            # ç®€å•çš„TCPè¿æ¥æµ‹è¯•
            try:
                reader, writer = await asyncio.wait_for(
                    asyncio.open_connection(server, port),
                    timeout=5.0
                )
                writer.close()
                await writer.wait_closed()

                latency = int((time.time() - start_time) * 1000)
                logger.debug(f"èŠ‚ç‚¹ {node.get('name')} å»¶è¿Ÿ: {latency}ms")
                return latency

            except asyncio.TimeoutError:
                logger.debug(f"èŠ‚ç‚¹ {node.get('name')} è¿æ¥è¶…æ—¶")
                return None

        except Exception as e:
            logger.debug(f"æµ‹è¯•èŠ‚ç‚¹å»¶è¿Ÿå¤±è´¥ {node.get('name')}: {e}")
            return None

    async def test_nodes_batch(self, nodes: List[Dict], batch_size: int = 50) -> List[Dict]:
        """
        æ‰¹é‡æµ‹è¯•èŠ‚ç‚¹

        Args:
            nodes: èŠ‚ç‚¹åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°

        Returns:
            å¯ç”¨èŠ‚ç‚¹åˆ—è¡¨
        """
        good_nodes = []

        for i in range(0, len(nodes), batch_size):
            batch = nodes[i:i + batch_size]
            logger.info(f"æµ‹è¯•æ‰¹æ¬¡ {i//batch_size + 1}: èŠ‚ç‚¹ {i+1}-{min(i+batch_size, len(nodes))}")

            tasks = []
            for node in batch:
                task = self.test_node_latency(node)
                tasks.append((node, task))

            # ç­‰å¾…æ‰€æœ‰æµ‹è¯•å®Œæˆ
            for node, task in tasks:
                try:
                    latency = await task
                    if latency is not None and latency <= self.max_latency:
                        node['latency'] = latency
                        good_nodes.append(node)
                        logger.info(f"âœ“ {node.get('name')} - {latency}ms")
                    else:
                        logger.warning(f"âœ— {node.get('name')} - å»¶è¿Ÿè¿‡é«˜æˆ–ä¸å¯è®¿é—®")
                except Exception as e:
                    logger.warning(f"âœ— {node.get('name')} - æµ‹è¯•å¼‚å¸¸: {e}")

            # æ‰¹æ¬¡é—´ä¼‘æ¯
            if i + batch_size < len(nodes):
                await asyncio.sleep(1)

        return good_nodes

    async def read_last_nodes(self, filename: str = 'good_nodes.json') -> List[Dict]:
        """
        è¯»å–ä¸Šæ¬¡ä¿å­˜çš„èŠ‚ç‚¹
        """
        if not self.should_keep_old:
            return []
        
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                return json.load(f)["nodes"]
        except Exception as e:
            logger.error(f"è¯»å–ä¸Šæ¬¡ä¿å­˜çš„èŠ‚ç‚¹å¤±è´¥: {e}")
            return []

    async def process_all_subscriptions(self) -> List[Dict]:
        """
        å¤„ç†æ‰€æœ‰è®¢é˜…

        Returns:
            æ‰€æœ‰å¯ç”¨èŠ‚ç‚¹åˆ—è¡¨
        """
        urls = self.read_config()
        if not urls:
            return []

        all_nodes = await self.read_last_nodes()
        logger.info(f"ä¸Šæ¬¡ä¿å­˜çš„èŠ‚ç‚¹: {len(all_nodes)} ä¸ª")

        for url in urls:
            nodes = await self.fetch_subscription(url)
            if nodes:
                all_nodes.extend(nodes)

        if not all_nodes:
            logger.warning("æœªè·å–åˆ°ä»»ä½•èŠ‚ç‚¹")
            return []

        logger.info(f"æ€»è®¡è·å–åˆ° {len(all_nodes)} ä¸ªèŠ‚ç‚¹ï¼Œå¼€å§‹æµ‹é€Ÿ...")

        # å»é‡
        unique_nodes = []
        seen = set()
        for node in all_nodes:
            key = (node.get('server'), node.get('port'), node.get('type'))
            if key not in seen:
                seen.add(key)
                unique_nodes.append(node)

        logger.info(f"å»é‡åå‰©ä½™ {len(unique_nodes)} ä¸ªèŠ‚ç‚¹")

        # æ‰¹é‡æµ‹è¯•
        good_nodes = await self.test_nodes_batch(unique_nodes)

        # æŒ‰å»¶è¿Ÿæ’åº
        good_nodes.sort(key=lambda x: x.get('latency', float('inf')))

        logger.info(f"ç­›é€‰å‡º {len(good_nodes)} ä¸ªå¯ç”¨èŠ‚ç‚¹")
        return good_nodes

    def generate_clash_config(self, nodes: List[Dict]) -> str:
        """
        ç”Ÿæˆæ ‡å‡†çš„Clashé…ç½®æ–‡ä»¶å†…å®¹

        Args:
            nodes: èŠ‚ç‚¹åˆ—è¡¨

        Returns:
            Clashé…ç½®æ–‡ä»¶å†…å®¹
        """
        # åŸºç¡€é…ç½®æ¨¡æ¿
        config = {
            'port': 7890,
            'socks-port': 7891,
            'allow-lan': False,
            'mode': 'Rule',
            'log-level': 'info',
            'external-controller': '127.0.0.1:9090',
            'dns': {
                'enable': True,
                'ipv6': False,
                'listen': '0.0.0.0:53',
                'enhanced-mode': 'fake-ip',
                'fake-ip-range': '198.18.0.1/16',
                'nameserver': [
                    '223.5.5.5',
                    '119.29.29.29',
                    '8.8.8.8'
                ],
                'fallback': [
                    'tls://1.1.1.1:853',
                    'tls://8.8.4.4:853'
                ]
            },
            'proxies': [],
            'proxy-groups': [
                {
                    'name': 'ğŸš€ èŠ‚ç‚¹é€‰æ‹©',
                    'type': 'select',
                    'proxies': ['â™»ï¸ è‡ªåŠ¨é€‰æ‹©', 'ğŸ”¯ æ•…éšœè½¬ç§»', 'ğŸ”® è´Ÿè½½å‡è¡¡', 'DIRECT']
                },
                {
                    'name': 'â™»ï¸ è‡ªåŠ¨é€‰æ‹©',
                    'type': 'url-test',
                    'proxies': [],
                    'url': 'http://www.gstatic.com/generate_204',
                    'interval': 300
                },
                {
                    'name': 'ğŸ”¯ æ•…éšœè½¬ç§»',
                    'type': 'fallback',
                    'proxies': [],
                    'url': 'http://www.gstatic.com/generate_204',
                    'interval': 300
                },
                {
                    'name': 'ğŸ”® è´Ÿè½½å‡è¡¡',
                    'type': 'load-balance',
                    'proxies': [],
                    'url': 'http://www.gstatic.com/generate_204',
                    'interval': 300
                }
            ],
            'rules': [
                'DOMAIN-SUFFIX,local,DIRECT',
                'IP-CIDR,127.0.0.0/8,DIRECT',
                'IP-CIDR,172.16.0.0/12,DIRECT',
                'IP-CIDR,192.168.0.0/16,DIRECT',
                'IP-CIDR,10.0.0.0/8,DIRECT',
                'IP-CIDR,17.0.0.0/8,DIRECT',
                'IP-CIDR,100.64.0.0/10,DIRECT',
                'DOMAIN-SUFFIX,cn,DIRECT',
                'GEOIP,CN,DIRECT',
                'MATCH,ğŸš€ èŠ‚ç‚¹é€‰æ‹©'
            ]
        }

        # æ¸…ç†èŠ‚ç‚¹æ•°æ®ï¼Œç§»é™¤å»¶è¿Ÿä¿¡æ¯ï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®
        proxy_names = []
        for node in nodes:
            # åˆ›å»ºèŠ‚ç‚¹å‰¯æœ¬å¹¶ç§»é™¤å»¶è¿Ÿä¿¡æ¯
            clean_node = {k: v for k, v in node.items() if k != 'latency'}

            # ç¡®ä¿èŠ‚ç‚¹åç§°å”¯ä¸€
            base_name = clean_node.get('name', 'Unknown')
            name = base_name
            counter = 1
            while name in proxy_names:
                name = f"{base_name}_{counter}"
                counter += 1
            clean_node['name'] = name
            proxy_names.append(name)

            # æ ¹æ®èŠ‚ç‚¹ç±»å‹è°ƒæ•´é…ç½®
            if clean_node.get('type') == 'ss':
                # ShadowsocksèŠ‚ç‚¹
                clean_node.setdefault('udp', True)
            elif clean_node.get('type') == 'vmess':
                # VMessèŠ‚ç‚¹
                clean_node.setdefault('network', 'tcp')
                clean_node.setdefault('cipher', 'auto')
                clean_node.setdefault('alterId', 0)
            elif clean_node.get('type') == 'trojan':
                # TrojanèŠ‚ç‚¹
                clean_node.setdefault('skip-cert-verify', False)
                clean_node.setdefault('udp', True)
            elif clean_node.get('type') == 'vless':
                # VLESSèŠ‚ç‚¹
                clean_node.setdefault('network', 'tcp')
                clean_node.setdefault('skip-cert-verify', False)

            config['proxies'].append(clean_node)

        # ä¸ºä»£ç†ç»„æ·»åŠ èŠ‚ç‚¹
        for group in config['proxy-groups']:
            if group['name'] in ['â™»ï¸ è‡ªåŠ¨é€‰æ‹©', 'ğŸ”¯ æ•…éšœè½¬ç§»', 'ğŸ”® è´Ÿè½½å‡è¡¡']:
                group['proxies'] = proxy_names.copy()

        # åœ¨èŠ‚ç‚¹é€‰æ‹©ç»„ä¸­æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹
        config['proxy-groups'][0]['proxies'].extend(proxy_names)

        # è½¬æ¢ä¸ºYAMLæ ¼å¼
        yaml_content = yaml.dump(config, default_flow_style=False, allow_unicode=True, sort_keys=False)

        # æ·»åŠ æ³¨é‡Šå¤´éƒ¨
        header = f"""# Clashé…ç½®æ–‡ä»¶
# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
# èŠ‚ç‚¹æ•°é‡: {len(nodes)}
# å¹³å‡å»¶è¿Ÿ: {sum(node.get('latency', 0) for node in nodes) / len(nodes):.1f}ms

"""

        return header + yaml_content

    async def upload_config_to_api(self, clash_config: str, api_url: str, api_key: str = None):
        """
        ä½¿ç”¨PUTæ–¹å¼å°†Clashé…ç½®ä¸Šä¼ åˆ°API

        Args:
            clash_config: Clashé…ç½®å†…å®¹
            api_url: APIåœ°å€
            api_key: APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰
        """
        try:
            headers = {'Content-Type': 'application/json'}
            if api_key:
                headers['Authorization'] = f'ApiKey {api_key}'

            data = {
                'content': clash_config
            }

            logger.info(f"ä½¿ç”¨PUTæ–¹å¼ä¸Šä¼ Clashé…ç½®åˆ°API: {api_url}")
            logger.info(f"é…ç½®å¤§å°: {len(clash_config)} å­—ç¬¦")

            async with self.session.put(api_url, json=data, headers=headers) as response:
                if response.status in [200, 201, 204]:
                    try:
                        result = await response.json()
                        logger.info(f"APIä¸Šä¼ æˆåŠŸ: {result}")
                    except:
                        logger.info("APIä¸Šä¼ æˆåŠŸ")
                else:
                    error_text = await response.text()
                    logger.error(f"APIä¸Šä¼ å¤±è´¥: HTTP {response.status} - {error_text}")

        except Exception as e:
            logger.error(f"APIä¸Šä¼ å¼‚å¸¸: {e}")

    def save_clash_config(self, clash_config: str, filename: str = 'clash_config.yaml'):
        """
        ä¿å­˜Clashé…ç½®åˆ°æ–‡ä»¶

        Args:
            clash_config: Clashé…ç½®å†…å®¹
            filename: æ–‡ä»¶å
        """
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(clash_config)

            logger.info(f"Clashé…ç½®å·²ä¿å­˜åˆ°æ–‡ä»¶: {filename}")

        except Exception as e:
            logger.error(f"ä¿å­˜Clashé…ç½®å¤±è´¥: {e}")

    def save_nodes_to_file(self, nodes: List[Dict], filename: str = 'good_nodes.json'):
        """
        ä¿å­˜èŠ‚ç‚¹åˆ°JSONæ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•å’Œå¤‡ä»½ï¼‰

        Args:
            nodes: èŠ‚ç‚¹åˆ—è¡¨
            filename: æ–‡ä»¶å
        """
        try:
            data = {
                'timestamp': datetime.now().isoformat(),
                'total_count': len(nodes),
                'average_latency': sum(node.get('latency', 0) for node in nodes) / len(nodes) if nodes else 0,
                'nodes': nodes
            }

            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            logger.info(f"èŠ‚ç‚¹æ•°æ®å·²ä¿å­˜åˆ°æ–‡ä»¶: {filename}")

        except Exception as e:
            logger.error(f"ä¿å­˜èŠ‚ç‚¹æ–‡ä»¶å¤±è´¥: {e}")

async def main():
    """ä¸»å‡½æ•°"""
    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
    config_file = os.getenv('CONFIG_FILE', 'config.txt')
    max_latency = int(os.getenv('MAX_LATENCY', '3000'))
    api_url = os.getenv('API_URL')
    api_key = os.getenv('API_KEY')
    output_file = os.getenv('OUTPUT_FILE', 'clash_config.yaml')
    should_keep_old = os.getenv('SHOULD_KEEP_OLD', false)

    logger.info("=== ClashèŠ‚ç‚¹æµ‹é€Ÿç­›é€‰ç³»ç»Ÿå¯åŠ¨ ===")
    logger.info(f"é…ç½®æ–‡ä»¶: {config_file}")
    logger.info(f"æœ€å¤§å»¶è¿Ÿ: {max_latency}ms")
    logger.info(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    logger.info(f"æ˜¯å¦ä¿ç•™æ—§é…ç½®: {should_keep_old}")

    async with ClashNodeTester(config_file, max_latency, should_keep_old) as tester:
        # å¤„ç†æ‰€æœ‰è®¢é˜…
        good_nodes = await tester.process_all_subscriptions()

        if not good_nodes:
            logger.warning("æœªæ‰¾åˆ°å¯ç”¨èŠ‚ç‚¹")
            return

        # ç”ŸæˆClashé…ç½®
        logger.info("ç”ŸæˆClashé…ç½®æ–‡ä»¶...")
        clash_config = tester.generate_clash_config(good_nodes)

        # ä¿å­˜Clashé…ç½®åˆ°æ–‡ä»¶
        tester.save_clash_config(clash_config, output_file)

        # ä¿å­˜èŠ‚ç‚¹æ•°æ®åˆ°JSONæ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        tester.save_nodes_to_file(good_nodes)

        # ä¸Šä¼ åˆ°APIï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        if api_url:
            await tester.upload_config_to_api(clash_config, api_url, api_key)
        else:
            logger.info("æœªé…ç½®APIåœ°å€ï¼Œè·³è¿‡ä¸Šä¼ ")

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        logger.info("=== å¤„ç†å®Œæˆ ===")
        logger.info(f"å¯ç”¨èŠ‚ç‚¹æ•°é‡: {len(good_nodes)}")
        if good_nodes:
            avg_latency = sum(node.get('latency', 0) for node in good_nodes) / len(good_nodes)
            logger.info(f"å¹³å‡å»¶è¿Ÿ: {avg_latency:.1f}ms")
            logger.info(f"æœ€ä½³èŠ‚ç‚¹: {good_nodes[0].get('name')} ({good_nodes[0].get('latency')}ms)")
            logger.info(f"Clashé…ç½®æ–‡ä»¶å¤§å°: {len(clash_config)} å­—ç¬¦")

if sys.platform == 'win32':
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

if __name__ == "__main__":

    asyncio.run(main())

